{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Chat_QA_chain_self import Chat_QA_chain_self #带历史记录的问答链\n",
    "from QA_chain_self import QA_chain_self       #不带历史记录的问答链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带历史记录的问答链  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "# model可选值：[\"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-0613\", \"gpt-4\", \"gpt-4-32k\"]，[\"ERNIE-Bot\", \"ERNIE-Bot-4\", \"ERNIE-Bot-turbo\"]，\n",
    "# [\"Spark-1.5\", \"Spark-2.0\"]，[\"chatglm_pro\", \"chatglm_std\", \"chatglm_lite\"]\n",
    "model:str = \"/data/xmx/model/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "temperature:float=0.0\n",
    "top_k:int=4\n",
    "chat_history:list=[] \n",
    "file_path:str = \"/data/xmx/project/DataWhale_Chat/knowledge_db\"\n",
    "persist_path:str = \"/data/xmx/project/DataWhale_Chat/vector_db/chroma\"\n",
    "appid:str=None \n",
    "api_key:str = \"\"   #or 从本地环境读取\n",
    "api_secret:str=None \n",
    "embedding = \"bge-m3\"     # [\"openai\",\"zhipuai\", \"m3e\"]  默认m3e\n",
    "embedding_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Chat_QA_chain_self.Chat_QA_chain_self object at 0x788fbc004690>\n"
     ]
    }
   ],
   "source": [
    "qa_chain = Chat_QA_chain_self(model=model, temperature=temperature, top_k=top_k, chat_history=chat_history, file_path=file_path, persist_path=persist_path, api_key=api_key, embedding = embedding, embedding_key=embedding_key)\n",
    "print(qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-03 09:06:56 [config.py:585] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 04-03 09:06:56 [arg_utils.py:1730] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
      "INFO 04-03 09:06:56 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 04-03 09:06:56 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.2) with config: model='/data/xmx/model/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='/data/xmx/model/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/xmx/model/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-03 09:06:56 [cuda.py:291] Using Flash Attention backend.\n",
      "INFO 04-03 09:06:57 [model_runner.py:1110] Starting to load model /data/xmx/model/DeepSeek-R1-Distill-Qwen-1.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.11s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-03 09:06:58 [loader.py:447] Loading weights took 1.25 seconds\n",
      "INFO 04-03 09:06:58 [model_runner.py:1146] Model loading took 3.3529 GB and 1.344959 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-03 09:06:59 [worker.py:267] Memory profiling takes 0.58 seconds\n",
      "INFO 04-03 09:06:59 [worker.py:267] the current vLLM instance can use total_gpu_memory (21.99GiB) x gpu_memory_utilization (0.90) = 19.79GiB\n",
      "INFO 04-03 09:06:59 [worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.01GiB; PyTorch activation peak memory takes 1.38GiB; the rest of the memory reserved for KV Cache is 15.05GiB.\n",
      "INFO 04-03 09:07:00 [executor_base.py:111] # cuda blocks: 35219, # CPU blocks: 9362\n",
      "INFO 04-03 09:07:00 [executor_base.py:116] Maximum concurrency for 131072 tokens per request: 4.30x\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 552.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 506.62 MiB is free. Process 3222808 has 13.87 GiB memory in use. Including non-PyTorch memory, this process has 7.60 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 239.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33m我可以学习到关于强化学习的知识吗？\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#answer,chat_history = qa_chain.answer(question)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m answer = \u001b[43mqa_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/xmx/project/DataWhale_Chat/qa_chain/Chat_QA_chain_self.py:76\u001b[39m, in \u001b[36mChat_QA_chain_self.answer\u001b[39m\u001b[34m(self, question, temperature, top_k)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m temperature == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     75\u001b[39m     temperature = \u001b[38;5;28mself\u001b[39m.temperature\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m llm = \u001b[43mmodel_to_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m#self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\u001b[39;00m\n\u001b[32m     80\u001b[39m retriever = \u001b[38;5;28mself\u001b[39m.vectordb.as_retriever(search_type=\u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m,   \n\u001b[32m     81\u001b[39m                                 search_kwargs={\u001b[33m'\u001b[39m\u001b[33mk\u001b[39m\u001b[33m'\u001b[39m: top_k})  \u001b[38;5;66;03m#默认similarity，k=4\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/xmx/project/DataWhale_Chat/qa_chain/model_to_llm.py:3\u001b[39m, in \u001b[36mmodel_to_llm\u001b[39m\u001b[34m(model, temperature)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmodel_to_llm\u001b[39m(model:\u001b[38;5;28mstr\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, temperature:\u001b[38;5;28mfloat\u001b[39m=\u001b[32m0.0\u001b[39m, appid:\u001b[38;5;28mstr\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, api_key:\u001b[38;5;28mstr\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m,Spark_api_secret:\u001b[38;5;28mstr\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m,Wenxin_secret_key:\u001b[38;5;28mstr\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         llm = VLLM(model=model)  \n\u001b[32m      4\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/pydantic/_internal/_decorators_v1.py:148\u001b[39m, in \u001b[36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[39m\u001b[34m(values, _)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/langchain_core/utils/pydantic.py:222\u001b[39m, in \u001b[36mpre_init.<locals>.wrapper\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m    219\u001b[39m             values[name] = field_info.default\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/langchain_community/llms/vllm.py:89\u001b[39m, in \u001b[36mVLLM.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import vllm python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install vllm`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m values[\u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mVLLModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensor_parallel_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrust_remote_code\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdownload_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvllm_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/utils.py:1037\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1030\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1032\u001b[39m         warnings.warn(\n\u001b[32m   1033\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1034\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1035\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/entrypoints/llm.py:243\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m engine_args = EngineArgs(\n\u001b[32m    215\u001b[39m     model=model,\n\u001b[32m    216\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     **kwargs,\n\u001b[32m    240\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/engine/llm_engine.py:520\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    518\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/engine/llm_engine.py:496\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    490\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    494\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    495\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_executor_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/engine/llm_engine.py:283\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = executor_class(vllm_config=vllm_config, )\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.runner_type != \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/engine/llm_engine.py:445\u001b[39m, in \u001b[36mLLMEngine._initialize_kv_caches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks = num_gpu_blocks\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_cpu_blocks = num_cpu_blocks\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m elapsed = time.time() - start\n\u001b[32m    447\u001b[39m logger.info((\u001b[33m\"\u001b[39m\u001b[33minit engine (profile, create kv cache, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    448\u001b[39m              \u001b[33m\"\u001b[39m\u001b[33mwarmup model) took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m), elapsed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/executor/executor_base.py:122\u001b[39m, in \u001b[36mExecutorBase.initialize_cache\u001b[39m\u001b[34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks = num_gpu_blocks\n\u001b[32m    120\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_config.num_cpu_blocks = num_cpu_blocks\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitialize_cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py:56\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     55\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/utils.py:2255\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2254\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/worker/worker.py:307\u001b[39m, in \u001b[36mWorker.initialize_cache\u001b[39m\u001b[34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[39m\n\u001b[32m    305\u001b[39m     context = nullcontext()\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_cache_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;28mself\u001b[39m._warm_up_model()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/worker/worker.py:312\u001b[39m, in \u001b[36mWorker._init_cache_engine\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache_engine = \u001b[43m[\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCacheEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipeline_parallel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28mself\u001b[39m.gpu_cache = [\n\u001b[32m    318\u001b[39m         \u001b[38;5;28mself\u001b[39m.cache_engine[ve].gpu_cache\n\u001b[32m    319\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.parallel_config.pipeline_parallel_size)\n\u001b[32m    320\u001b[39m     ]\n\u001b[32m    321\u001b[39m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m.compilation_config.static_forward_context,\n\u001b[32m    322\u001b[39m                   \u001b[38;5;28mself\u001b[39m.gpu_cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/worker/worker.py:313\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache_engine = [\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m         \u001b[43mCacheEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.parallel_config.pipeline_parallel_size)\n\u001b[32m    316\u001b[39m     ]\n\u001b[32m    317\u001b[39m     \u001b[38;5;28mself\u001b[39m.gpu_cache = [\n\u001b[32m    318\u001b[39m         \u001b[38;5;28mself\u001b[39m.cache_engine[ve].gpu_cache\n\u001b[32m    319\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.parallel_config.pipeline_parallel_size)\n\u001b[32m    320\u001b[39m     ]\n\u001b[32m    321\u001b[39m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m.compilation_config.static_forward_context,\n\u001b[32m    322\u001b[39m                   \u001b[38;5;28mself\u001b[39m.gpu_cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/worker/cache_engine.py:64\u001b[39m, in \u001b[36mCacheEngine.__init__\u001b[39m\u001b[34m(self, cache_config, model_config, parallel_config, device_config)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.attn_backend = get_attn_backend(\u001b[38;5;28mself\u001b[39m.head_size,\n\u001b[32m     57\u001b[39m                                      model_config.dtype,\n\u001b[32m     58\u001b[39m                                      cache_config.cache_dtype,\n\u001b[32m     59\u001b[39m                                      \u001b[38;5;28mself\u001b[39m.block_size,\n\u001b[32m     60\u001b[39m                                      model_config.is_attention_free,\n\u001b[32m     61\u001b[39m                                      use_mla=model_config.use_mla)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Initialize the cache.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28mself\u001b[39m.gpu_cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_allocate_kv_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.cpu_cache = \u001b[38;5;28mself\u001b[39m._allocate_kv_cache(\u001b[38;5;28mself\u001b[39m.num_cpu_blocks, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/chat/lib/python3.11/site-packages/vllm/worker/cache_engine.py:83\u001b[39m, in \u001b[36mCacheEngine._allocate_kv_cache\u001b[39m\u001b[34m(self, num_blocks, device)\u001b[39m\n\u001b[32m     77\u001b[39m kv_cache: List[torch.Tensor] = []\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_attention_layers):\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# null block in CpuGpuBlockAllocator requires at least that\u001b[39;00m\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# block to be zeroed-out.\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# We zero-out everything for simplicity.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     layer_kv_cache = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# view back to (TOTAL_PAGES, PAGE_SIZE, entry_shape...) for cases\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# when entry_shape is higher than 1D\u001b[39;00m\n\u001b[32m     90\u001b[39m     kv_cache.append(layer_kv_cache)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 552.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 506.62 MiB is free. Process 3222808 has 13.87 GiB memory in use. Including non-PyTorch memory, this process has 7.60 GiB memory in use. Of the allocated memory 7.09 GiB is allocated by PyTorch, and 239.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "question = \"我可以学习到关于强化学习的知识吗？\"\n",
    "#answer,chat_history = qa_chain.answer(question)\n",
    "answer = qa_chain.answer(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('我可以学习到关于强化学习的知识吗？', '根据您提供的信息，强化学习是一种机器学习的子领域，主要研究在不确定的情况下如何做出好的决策。强化学习应用广泛，如玩游戏、控制机器人、自动驾驶等。您提到的《Easy RL：强化学习教程》是一本关于强化学习的教材，该书通过一些简单生动的例子来解释强化学习概念，配有对应的概念词、习题和面试题，以及代码实战。这本书得到了一些专业人士的好评，并被推荐用于学习强化学习。\\\\n\\\\n另外，您还提到了一个名为rl-papers的仓库，该仓库收集了强化学习及其应用领域的论文，涵盖了基础强化学习、多智体强化学习、推荐系统、游戏理论、交通系统等多个方面。通过阅读这些论文，您也可以了解到关于强化学习的知识。\\\\n\\\\n综上所述，您可以学习到关于强化学习的知识，可以通过阅读《Easy RL：强化学习教程》和参考rl-papers仓库中的论文来进一步了解强化学习。')]\n",
      "[('我可以学习到关于强化学习的知识吗？', '根据您提供的信息，强化学习是一种机器学习的子领域，主要研究在不确定的情况下如何做出好的决策。强化学习应用广泛，如玩游戏、控制机器人、自动驾驶等。您提到的《Easy RL：强化学习教程》是一本关于强化学习的教材，该书通过一些简单生动的例子来解释强化学习概念，配有对应的概念词、习题和面试题，以及代码实战。这本书得到了一些专业人士的好评，并被推荐用于学习强化学习。\\\\n\\\\n另外，您还提到了一个名为rl-papers的仓库，该仓库收集了强化学习及其应用领域的论文，涵盖了基础强化学习、多智体强化学习、推荐系统、游戏理论、交通系统等多个方面。通过阅读这些论文，您也可以了解到关于强化学习的知识。\\\\n\\\\n综上所述，您可以学习到关于强化学习的知识，可以通过阅读《Easy RL：强化学习教程》和参考rl-papers仓库中的论文来进一步了解强化学习。')]\n"
     ]
    }
   ],
   "source": [
    "print(answer)\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"为什么这门课需要教这方面的知识？\"\n",
    "answer,chat_history = qa_chain.answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('我可以学习到关于强化学习的知识吗？', '根据您提供的信息，强化学习是一种机器学习的子领域，主要研究在不确定的情况下如何做出好的决策。强化学习应用广泛，如玩游戏、控制机器人、自动驾驶等。您提到的《Easy RL：强化学习教程》是一本关于强化学习的教材，该书通过一些简单生动的例子来解释强化学习概念，配有对应的概念词、习题和面试题，以及代码实战。这本书得到了一些专业人士的好评，并被推荐用于学习强化学习。\\\\n\\\\n另外，您还提到了一个名为rl-papers的仓库，该仓库收集了强化学习及其应用领域的论文，涵盖了基础强化学习、多智体强化学习、推荐系统、游戏理论、交通系统等多个方面。通过阅读这些论文，您也可以了解到关于强化学习的知识。\\\\n\\\\n综上所述，您可以学习到关于强化学习的知识，可以通过阅读《Easy RL：强化学习教程》和参考rl-papers仓库中的论文来进一步了解强化学习。')\n",
      "('为什么这门课需要教这方面的知识？', '强化学习作为人工智能的一个重要分支，主要研究在不确定的情况下如何做出好的决策。现实生活中许多问题都具有不确定性和动态性，因此强化学习在很多领域都有广泛的应用，如自动驾驶、机器人控制、智能交通、游戏AI等。学习强化学习可以帮助学生更好地理解和解决这些问题，为我国人工智能领域的发展贡献力量。此外，强化学习也是目前学术界和工业界的研究热点，掌握这门课程的知识可以帮助学生在未来的工作和研究中更好地适应和引领这个领域的发展。')\n"
     ]
    }
   ],
   "source": [
    "print(answer)\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变历史记录的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('为什么这门课需要教这方面的知识？', '强化学习作为人工智能的一个重要分支，主要研究在不确定的情况下如何做出好的决策。现实生活中许多问题都具有不确定性和动态性，因此强化学习在很多领域都有广泛的应用，如自动驾驶、机器人控制、智能交通、游戏AI等。学习强化学习可以帮助学生更好地理解和解决这些问题，为我国人工智能领域的发展贡献力量。此外，强化学习也是目前学术界和工业界的研究热点，掌握这门课程的知识可以帮助学生在未来的工作和研究中更好地适应和引领这个领域的发展。')]\n"
     ]
    }
   ],
   "source": [
    "history_len = 1 \n",
    "\n",
    "### 改变历史记录的长度,使用的是history_len参数来改变保留历史记录的长度\n",
    "chat_history = qa_chain.change_history_length(history_len)\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清除历史记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "chat_history = qa_chain.clear_history()\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不带历史记录的问答链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "# model可选值：[\"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-0613\", \"gpt-4\", \"gpt-4-32k\"]，[\"ERNIE-Bot\", \"ERNIE-Bot-4\", \"ERNIE-Bot-turbo\"]，\n",
    "# [\"Spark-1.5\", \"Spark-2.0\"]，[\"chatglm_pro\", \"chatglm_std\", \"chatglm_lite\"]\n",
    "model:str = \"chatglm_std\"\n",
    "temperature:float=0.0\n",
    "top_k:int=4 \n",
    "file_path:str = \"/Users/lta/Desktop/llm-universe/data_base/knowledge_db\"\n",
    "persist_path:str = \"/Users/lta/Desktop/llm-universe/data_base/vector_db/chroma\"\n",
    "appid:str=None \n",
    "api_key:str = \"\"   #or从本地环境读取\n",
    "api_secret:str=None \n",
    "embedding = \"m3e\"\n",
    "embedding_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<QA_chain_self.QA_chain_self object at 0x2f5afdf10>\n"
     ]
    }
   ],
   "source": [
    "#星火输入参数用法\n",
    "#qa_chain = QA_chain_self(model=model, temperature=temperature, top_k=top_k, file_path=file_path, persist_path=persist_path, appid=appid,api_key=api_key,Spark_api_secret=Spark_api_secret, embedding = embedding,embedding_key=embedding_key)\n",
    "\n",
    "##百度文心输入参数用法\n",
    "#qa_chain = QA_chain_self(model=model, temperature=temperature, top_k=top_k, file_path=file_path, persist_path=persist_path, appid=appid,api_key=api_key,Wenxin_api_secret=Wenxin_api_secret, embedding = embedding,embedding_key=embedding_key)\n",
    "\n",
    "#智谱(或OpenAI)输入参数用法\n",
    "qa_chain = QA_chain_self(model=model, temperature=temperature, top_k=top_k, file_path=file_path, persist_path=persist_path, appid=appid,api_key=api_key, embedding = embedding,embedding_key=embedding_key)\n",
    "print(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"什么是蘑菇书（easyrl）？\"\n",
    "answer = qa_chain.answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蘑菇书（easy-rl）是一本关于强化学习的教程，主要包含李宏毅老师的强化学习视频内容、经典资料整理、章节习题和算法实战等，旨在帮助读者学习强化学习并探索人工智能领域。谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "> 其他模型可参考上面的实例进行使用，embedding 目前支持 openAI 和智谱以及m3e的模型，其他类型敬请期待！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
